# @package _global_

# Smoke test configuration for DPLM-150M
# This config is designed for quick testing with minimal compute requirements
# Run with: bash scripts/run_smoke_test.sh

defaults:
  - /datamodule: uniref50_hf
  - /callbacks: lm
  - /trainer: default  # Use single GPU, no distributed training

# name of the run determines folder name in logs
project: "DPLM_Smoke_Test"
name: "dplm_150m_smoke_test"

datamodule:
  data_dir: ${paths.data_dir}/uniref50_1k  # Tiny 1K subset for smoke test
  max_tokens: 512     # Small batch size for quick iteration
  max_len: 256        # Shorter sequences for faster training
  num_workers: 2      # Fewer workers for smoke test

model:
  _target_: dplm
  num_diffusion_timesteps: 500
  gradient_ckpt: false  # Disable for faster training in smoke test
  rdm_couple: false
  lora:
    enable: false
    lora_rank: 16
    lora_dropout: 0.1
    lora_target_module: (esm.encoder.layer.[0-9]*.attention.(self.query|self.key|self.value|output.dense).*|esm.encoder.layer.[0-9]*.(intermediate|output).dense.*)
    modules_to_save: lm_head,esm.embeddings
  net:
    arch_type: esm
    name: facebook/esm2_t12_35M_UR50D  # Use smaller 35M model for ultra-fast smoke test
    dropout: 0.1

task:
  _target_: lm/dplm
  learning:
    noise: random_mask
    watch_t1_t2_loss: false
    cal_constant_loss: false
    weight: linear
  criterion:
    _target_: byprot.modules.cross_entropy.RDMCrossEntropyLoss
    label_smoothing: 0.0
    ignore_index: 1
  optimizer:
    type: adamw
    _partial_: true
    lr: ${train.lr}
    betas:
      - 0.9
      - 0.98
    weight_decay: 0.01
  lr_scheduler:
    type: polynomial
    warmup_steps: 10      # Very short warmup for smoke test
    total_steps: ${trainer.max_steps}
    lr: ${train.lr}
    lr_end: 1e-5
    warmup_init_lr: 1e-07
    power: 1

train:
  seed: 42
  lr: 0.0001  # Slightly higher LR for faster convergence in smoke test
  monitor: "train/loss"
  mode: "min"
  patience: 1000

trainer:
  min_epochs: 1
  max_epochs: 100
  gradient_clip_val: 1.0
  num_sanity_val_steps: 1  # Quick sanity check
  reload_dataloaders_every_n_epochs: 1
  use_distributed_sampler: false
  max_steps: 100           # Only 100 steps for smoke test
  accumulate_grad_batches: 1
  check_val_every_n_epoch: null
  val_check_interval: 20   # Validate every 20 steps
  enable_progress_bar: true
  num_nodes: 1
  devices: 1               # Single GPU
  precision: 16            # FP16 for speed

# Logging
logger:
  tensorboard:
    save_dir: "${paths.log_dir}/tensorboard"
    name: "smoke_test"
    default_hp_metric: false

callbacks:
  model_checkpoint:
    dirpath: ${paths.ckpt_dir}
    filename: "step_{step:06d}"
    monitor: "train/loss"
    mode: "min"
    save_last: true
    save_top_k: 1
    every_n_train_steps: 50  # Save checkpoint every 50 steps

